# LLM Prompt Evaluation

Small toolkit to run prompt tests against different LLMs and collect structured outputs for human evaluation.

Files:
- evaluate.py: example runner that simulates LLM calls and writes JSON outputs.
